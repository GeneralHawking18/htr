optimizer:
    type: adam
    lr: 1e-4
    betas: [0.9, 0.98]
    eps: 1e-9
    weight_decay: 0.01
    lr_mul: 0.5
    n_warm_steps: 25000
    n_steps: 684 * 31 # 684 * (48 + 100)
        
OneCycleLR:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    pct_start: 0.01
    max_lr: 2e-4
    div_factor: 4
    # div_factor: 60 # min_lr = max_lr / div_factor
    # final_div_factor: 1e3
    three_phase: False

CyclicLR:
    _target_: torch.optim.lr_scheduler.CyclicLR
    base_lr: 4.0e-5
    max_lr: 1.0e-4
    base_momentum: 0.85
    max_momentum: 0.95
    scale_mode: 'iterations'
    mode: 'exp_range'
    gamma: 1 - 4e-4
    step_size_up: 2
    step_size_down: 98
    cycle_momentum: false
        
ReduceLROnPlateau:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: 'min'
    factor: 0.2
    verbose: True
    
# CyclicLR:
    
